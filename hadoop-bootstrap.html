---
layout: default
title: SenseiDB - Hadoop Bootstrap
---
<div class="hero-unit">
  <h2>Sensei &amp; Hadoop</h2>
  <p>Batch indexing on Hadoop, load into Sensei and run queries.</p>
</div>

<h2>Hadoop</h2>
<p>
  <a href="http://hadoop.apache.org/">Apache Hadoop</a> has become an important part of data infrastructure solutions in the industry. In enterprises, Hadoop is where large amounts of data are stored, aggregated and transformed via Map-Reduce jobs. 
</p>
<p>
  For convenience and efficiency, it is a good idea to let Hadoop perform batch-indexing by leveraging its storage and parallelized computation capacities. 
</p>

<h2>Sensei Hadoop integration</h2>
<p>
  We have written a fast Map-Reduce job by taking data from Hadoop and batch build indexes given a Sensei schema and sharding strategy.
</p>
<p>The following diagram illustrates this indexing process:</p>
<img src="images/sensei-hadoop-index.png" />


<h3>Layout of source packages</h3>
<p>
We explain the functionality of Sensei Hadoop indexing and its workflow through the layout of sensei hadoop indexing source packages:
  <ul>
    <li><b>com.sensei.indexing.hadoop.job;</b></li>
      <p>This package contains the job configuration file loading classes.</p>
  </ul>

  <ul>
    <li><b>com.sensei.indexing.hadoop.keyvalueformat;</b></li>
      <p>This package specifies the data format (key, value) used in hadoop-indexing system.</p>
  </ul>

  <ul>
    <li><b>com.sensei.indexing.hadoop.map;</b></li>
      <p>This package contains underlying Sensei Hadoop Mapper file and also a dummy Converter to deal with preprocess of input record.</p>
  </ul>

  <ul>
    <li><b>com.sensei.indexing.hadoop.reduce;</b></li>
      <p>This package contains Hadoop Reduer and Combiner files.</p>
  </ul>

  <ul>
    <li><b>com.sensei.indexing.hadoop.util;</b></li>
      <p>Util package has all kinds of naming configuration files and utility tools.</p>
  </ul>
  
</p>


<h3>Car Demo</h3>
<p>
To be consistent, we also use the car demo data to describe our hadoop indexing system. Sensei package comes with a car demo hadoop-indexing example and it mainly has four simple files. The first one "CarDemo.java" is a simple main class providing a starting point for running the program. "CarMapInputConverter.java" is a sample converter class to convert the input data record for map job. "CarShardingStrategy.java" is another class to provide a user-specified sharding strategy. Also we need a configuration file to help Sensei Hadoop indexer to configure all the rest properties, a sample configuration file can be found at "example/hadoop-indexing/conf/JobCarDemo.job".

To run the demo, simple compile and package the project into an excutable jar file called CarDemo.jar. Then run "hadoop jar CarDemo.jar".

(You need to specify the job configuration file location in CarDemo.java, and upload the schema file into the HDFS system before running, and also the location of the schema file should be set in the configuration file)
</p>

<h4>(1) Use hadoop to build index</h4>
<p>To build index through Sensei-hadoop-indexer, simply write your own job files just as the <a href="https://github.com/javasoze/sensei/tree/json-api/example/hadoop-indexing/src/main/java/com/sensei/indexing/hadoop/demo">car demo files</a> (java files with main function as entry point, and also specifying where the configuration file is, sharding strategy, etc.), and then package it as a runnable jar file (e.g., example.jar) to execute:</p>
<pre class="prettyprint" id="xml">
<?xml version="1.0" encoding="UTF-8"?>
hadoop jar example.jar
 
</pre>

<p>Car demo configuration:</p>
<pre class="prettyprint" id="xml">
<?xml version="1.0" encoding="UTF-8"?>
type=java
job.class=com.sensei.indexing.hadoop.demo.CarDemo

mapreduce.job.maps=2
sensei.num.shards=3

mapred.job.name=CarDemoShardedIndexing

# if the output.path already exists, delete it first
sensei.force.output.overwrite=true

# adjust this to a small one if mapper number is huge. default is 50Mb =  52428800
sensei.max.ramsize.bytes=52428800

#############   path of schema for interpreter #############

##### TextJSON schema Sample (car demo) absolute path ######
sensei.schema.file.url=conf/schema.xml

############    Input and Output  ##################

####### Text JSON data (car demo) #####
read.lock=data/cars.json
sensei.input.dirs=data/cars.json

######## Output configuration ######
write.lock=example/hadoop-indexing/output
sensei.output.dir=example/hadoop-indexing/fileoutput

######## Index output location ######
sensei.index.path=example/hadoop-indexing/index

############# schemas for mapper input  ################

sensei.input.format=org.apache.hadoop.mapred.TextInputFormat

##############  Sharding strategy  ################
sensei.distribution.policy=com.sensei.indexing.hadoop.demo.CarShardingStrategy

#############  Converter for mapper input (data conversion and filtering) ##########
sensei.mapinput.converter=com.sensei.indexing.hadoop.demo.CarMapInputConverter

#############  Analyzer configuration for lucene ###############
sensei.document.analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
sensei.document.analyzer.version=LUCENE_30
 
</pre>

<h4>(2) Bootstrap a running Sensei server from index built by hadoop</h4>
<p>To bootstrap Sensei index, we firstly start a Sensei server (remember to add "sensei.indexer.copier=hdfs" property in the sensei.properties file):</p>
<pre class="prettyprint" id="xml">
<?xml version="1.0" encoding="UTF-8"?>
./bin/start-sensei-node.sh example/cars/conf/
 
</pre>

<p>After Sensei is started, we specify the location and port of the running Sensei Server, and also the index location in HDFS built in the first step. 
We bootstrap this server by doing the folloing:</p>
<pre class="prettyprint" id="xml">
<?xml version="1.0" encoding="UTF-8"?>
./bin/load-index -s localhost -p 8080 hdfs://esv4-heartsnn01.corp.linkedin.com:9000/user/your-acount-name/example/hadoop-indexing/index
 
</pre>



<h2>Data warehousing</h2>
<p>
  Other than this conveninence, there are some other immediate benefits, for example: data-warehousing.
</p>
<p>
  Traditionally, data-warehousing solutions are built on the RDBMS. In the curret era of information explosion, traditional solutions are no longer feasible given the amount of data.
</p>
<p>
  Recently, technologies such as Apache's <a href="http://pig.apache.org/">Pig</a> and <a href="http://hive.apache.org/">Hive</a> projects have been developed in solving this problem by translation a SQL-like query language into Map-Reduce jobs over Hadoop. This made data-warehousing on large datasets possible.
</p>
<p>
  In scenarios where data-scientists work on a set of data files for some project, the underlying dataset do not change between queries issued by subsequent Pig/Hive scripts into Hadoop, which means much of the work is redundant and wasteful. It would make sense by writing a data-preparation job by aggregating all parts of data of interest, and launch the batch indexing job to produce Sensei shards. Once loaded into Sensei, fast queries over BQL can be executed and this avoids paying a Map-Reduce cost per query.
</p>


<h2>Javadoc</h2>
<p>For Javadoc reference, go to: <a class="btn primary" href="javadoc/sensei-hadoop-indexing/apidocs/index.html" target="_blank">Javadoc &raquo;</a>